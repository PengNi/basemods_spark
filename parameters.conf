[FilePath]
# the installation home of SMRT-Analysis
SMRT_ANALYSIS_HOME = /home/smrtanalysis/smrtana/install/smrtanalysis_2.3.0.140936

# directory for save the tmp data
# Note that the dir must have enough space.
TEMP_OUTPUT_FOLDER = /tmp/basemods_spark_data

# the reference file directory in your master node
REFERENCE_DIR = /home/hadoop/workspace_py/basemods_spark/data/lambda/sequence
REF_FILENAME = lambda.fasta
REF_SA_FILENAME = lambda.fasta.sa

# the smrt cell data directory in your master node
# If you use the disk mode-"basemods_spark_runner_disk.py", you must copy all the data
# to the same directory of EACH worker node in your cluster.
CELL_DATA_DIR = /home/hadoop/workspace_py/basemods_spark/data/lambda_v210


[PipelineArgs]
# the number of cores you allow each SMRT-Analysis to use
# It's ok to set CORE_NUM to 40 if each worker node has 40 cpu cores.
CORE_NUM = 3

# the folds of each bax.h5 file you want to split to
BAXH5_FOLDS = 1

# It is better that REF_CHUNKS_FACTOR is no greater than CORE_NUM.
# replaced by spark_task_cpus
# REF_CHUNKS_FACTOR = 2

# maxCoverage in ipdSummary.py
IPDMAXCOVERAGE = 300

# methylation types to be identified, for now there are three kinds: "m6A,m5C,m4C"
# Use ',' as delimiter. No space allowed.
METHYLATION_TYPES = m6A,m4C


[MasterNodeInfo]
# hostname or IP of your master node
HOST = 127.0.0.1

# host port
HOSTPORT = 22

# user name to access your master node
USERNAME = hadoop

# user password
USERPASSWD = hadoop


# work nodes' info for scp
[WorkerNodeInfo]
# ssh port
WORKERNODE_PORT = 22

# user name to access your worker nodes
USERNAME = hadoop

# user password to access your worker nodes
USERPASSWD = 123


[SparkConfiguration]
# should be no greater than the memory of a worker node
spark_executor_memory = 4g

# 2, 4 or 5?
spark_task_cpus = 2

# default value in Spark Configuration is 0.6
spark_memory_fraction = 0.6

# default value in Spark Configuration is 0.5
spark_memory_storageFraction = 0.5
